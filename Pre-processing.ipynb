{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a919dd37",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "\n",
    "Description: The script will pre-process the data. This step of pre-processing will include: removing rows with missing values, binary encoding categorical data\n",
    "\n",
    "Author: Caroline Risoud\n",
    "\n",
    "License:\n",
    "\n",
    "Last update date: 23.10.2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9ae598",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18868/2088547898.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfdbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv data file\n",
    "df_train_validate = pd.read_csv('nhts_train_validate.csv', index_col='TRIPID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76318aba",
   "metadata": {},
   "source": [
    "### Missing values in the dataset\n",
    "\n",
    "In the following cell:\n",
    "\n",
    "- We create a dataframe 'df_null_values' which will list all the columns (features, labels) of the dataset and will show how many NULL (None, NaN) values each of the columns counts.\n",
    "\n",
    "- We see that NULL values (exactly 8) are only found in the TRAVELMODE column which is the target/label column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9addb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the amount of NULL values\n",
    "\n",
    "# For each column, total stores the number of null values\n",
    "total = df_train_validate.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# For each column, percent translater the number of null values into a percentage\n",
    "percent = total/len(df_train_validate)*100\n",
    "\n",
    "\n",
    "# Dataframe with Total as the first column and Percent as the second one\n",
    "df_null_values = pd.concat([total,percent], axis=1, keys=['Total', 'Percent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cd71a3",
   "metadata": {},
   "source": [
    "- We will drop the 8 rows with missing TRAVELMODE. We assume that these row removals will not introduce sampling bias because. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_validate = df_train_validate.dropna(how='any', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba624905",
   "metadata": {},
   "source": [
    "### Categorical data - Binary encoding\n",
    "\n",
    "All data must be numerical.\n",
    "\n",
    "For our target label, TRAVELMODE, we choose numerical encoding. Indeed, we want to have one column for the target categorical value which leads to the use of linear encoding to achieve that.\n",
    "\n",
    "For the features, we choose One-hot encoding (Binary encoding) as a solution. On one hand, this solution does not impute an order and distance compared to a numerical encoding. On the other hand, it makes the data much wider by creating many more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca867aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows us the type of each column - the object types are the ones to be encoded\n",
    "\n",
    "display(df_train_validate.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f065bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical encoding for the target categorical lable TRAVELMODE\n",
    "\n",
    "str_to_val = {\n",
    "    'WALK': 0,\n",
    "    'CYCLE': 1,\n",
    "    'RAIL': 2,\n",
    "    'BUS': 3,\n",
    "    'DRIVE': 4,\n",
    "    'PASSENGER': 5,\n",
    "    'TAXI': 6,\n",
    "    'OTHER': 7\n",
    "}\n",
    "\n",
    "# Replacing the strings with their respective values\n",
    "df_train_validate.TRAVELMODE.replace(str_to_val, inplace=True)\n",
    "\n",
    "df_train_validate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeee956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding for the 4 remaining categorical features: 'TRIPPURP','HHSTATE', 'OBHUR', 'DBHUR'\n",
    "\n",
    "categorical_cols = [\n",
    "    'TRIPPURP',\n",
    "    'HHSTATE',\n",
    "    'OBHUR',\n",
    "    'DBHUR'\n",
    "]\n",
    "\n",
    "\n",
    "df_processed = pd.get_dummies(\n",
    "    df_train_validate, prefix_sep=':', columns=categorical_cols)\n",
    "\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab46ad",
   "metadata": {},
   "source": [
    "### Internal and external validation\n",
    "\n",
    " The test data here can not be used for external validation because it DOES NOT include the choice label.\n",
    " \n",
    "In order to create a test set that represents external validation, we will seperate the train_validate set into train_validation and test set. Cross-validation to optimize the hyperparameters will be carried out on the train_validate set and finally the test set can be used for external validation. \n",
    " \n",
    "    train_validate data:\n",
    "        --> 80 % train_validate_rev\n",
    "        --> 20 % test_rev\n",
    " \n",
    "\n",
    "The Cross-validation with random search will be used to optimise the hyperparameters, it will also account for the hierarchical nature of the data. It will be used as follows:\n",
    "\n",
    " - Train on 4 folds, test on 1 fold\n",
    " - Training data: 80% of the train_validate_rev\n",
    " - Test data: 20% of the train_validate_rev\n",
    " \n",
    " Random sampling of validation folds\n",
    " --> INTERNAL VALIDATION\n",
    " \n",
    "     \n",
    " The Test:\n",
    " \n",
    " - Training data: 100% of the train_validate_rev data with the optimal hyperparameters found previously\n",
    " - Test data: 100% of the test_rev\n",
    " \n",
    " --> EXTERNAL VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b9f7d",
   "metadata": {},
   "source": [
    "Splitting train_validate dataframe by row index into train and validate dataframe:\n",
    "\n",
    "- 80% of the df_train_validate is assigned to df_train_validate_rev\n",
    "- 20% of the df_train_validate is assigned to df_test_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining index 1\n",
    "id1 = round(len(df_processed)*0.8)\n",
    "\n",
    "df_train_validate_rev = df_processed.iloc[:id1,:]\n",
    "df_test_rev = df_processed.iloc[id1:,:]\n",
    "\n",
    "print(\"Shape of new dataframes - {} , {}\".format(df_train_validate_rev.shape, df_test_rev.shape))\n",
    "\n",
    "df_processed.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25beb3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract the features and labels, removing the id and context columns\n",
    "\n",
    "target = ['TRAVELMODE']\n",
    "\n",
    "id_context = ['TRIPID', \n",
    "              'HOUSEID', \n",
    "              'PERSONID', \n",
    "              'TDTRPNUM',\n",
    "              'LOOP_TRIP'\n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "features = [c for c in df_processed.columns \n",
    "            if c not in (target + id_context)]\n",
    "\n",
    "# y is the target label (here: 'TRAVELMODE')\n",
    "# X are the features that are inputed to the model to predict the target label\n",
    "\n",
    "# ravel() is used to flatten the multi-dimensional array to a vector\n",
    "y = df_processed[target].values.ravel()\n",
    "X = df_processed[features]\n",
    "\n",
    "y_train_validate_rev = df_train_validate_rev[target].values.ravel()\n",
    "X_train_validate_rev = df_train_validate_rev[features]\n",
    "\n",
    "y_test_rev = df_test_rev[target].values.ravel()\n",
    "X_test_rev = df_test_rev[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1304efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store X_train_validate_rev, y_train_validate_rev\n",
    "%store X_test_rev, y_test_rev"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
